{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f9aac37-9fcb-4957-b0ca-2505e2c71cda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Preparación datos - Tablero Construcción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aeebc16-617f-40b4-8f40-0cd7f0c32f28",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Instalación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a58f5852-2866-4254-b644-cf02045d2910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install geopandas\n",
    "# !pip install matplotlib\n",
    "# !pip install requests\n",
    "# !pip install swifter\n",
    "# !pip install pyxlsb\n",
    "# !pip install openpyxl\n",
    "\n",
    "# !pip install --upgrade pip\n",
    "# !pip install pyxlsb\n",
    "# !pip install pandas\n",
    "# !pip install geopandas\n",
    "\n",
    "# !pip install azure-storage-blob\n",
    "\n",
    "# !pip install pyproj\n",
    "\n",
    "# !pip install fuzzywuzzy\n",
    "# !pip install python-Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f642dcc-c9f4-4da5-8bb0-72977b8aca47",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyproj\n",
    "import geopandas as gpd\n",
    "import matplotlib\n",
    "\n",
    "import geopandas as gpd\n",
    "from shapely import wkt\n",
    "\n",
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.wkt import loads\n",
    "\n",
    "import unicodedata\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "import jenkspy\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col, cast, when, udf, struct, from_json, count, countDistinct, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType, LongType\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "from sedona.spark import *\n",
    "from sedona.utils import SedonaKryoRegistrator, KryoSerializer\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from sedona.core.formatMapper.shapefileParser import ShapefileReader\n",
    "from sedona.utils.adapter import Adapter\n",
    "from sedona.core.enums import IndexType\n",
    "from sedona.core.enums import GridType\n",
    "from sedona.core.spatialOperator import JoinQuery\n",
    "\n",
    "from sedona.core.SpatialRDD import PointRDD\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67043975-f892-49cf-ae23-a49b0f01b83a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Parametrización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c692b488-095e-4ef5-b208-dca8dae3fc61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "num_partitions = 4\n",
    "url = \"https://catalogopmb.catastrobogota.gov.co/PMBWeb/web/api\"\n",
    "startpath = '/dbfs/FileStore/tables/Catastro/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8afe834f-3f02-4f62-b84c-1d3565d6a53a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# configuración de spark y Sedona para datos geograficos\n",
    "spark = SparkSession.\\\n",
    "    builder.\\\n",
    "    master(\"local[*]\").\\\n",
    "    appName(\"Sedona App\").\\\n",
    "    config(\"spark.serializer\", KryoSerializer.getName).\\\n",
    "    config(\"spark.kryo.registrator\", SedonaKryoRegistrator.getName) .\\\n",
    "    config(\"spark.kryoserializer.buffer.max\", \"200gb\").\\\n",
    "    getOrCreate()\n",
    "\n",
    "SedonaContext.create(spark)\n",
    "\n",
    "\n",
    "sc = spark.sparkContext\n",
    "sc.setSystemProperty(\"sedona.global.charset\", \"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1855450-b26c-4af0-ac40-7435dfa6722c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usos_homologados={\n",
    "    'Asociar al uso principal':['AREA DE MEZANINE EN PH','DEPOSITO (LOCKERS) PH','DEPOSITO ALMACENAMIENTO PH','PARQUEO CUBIERTO NPH','PARQUEO CUBIERTO PH','PARQUEO LIBRE PH','PISCINAS EN NPH','PISCINAS EN PH','SECADEROS'],\n",
    "    'Comercio y servicios':['BODEGA COMERCIAL NPH','BODEGA COMERCIAL PH','BODEGA ECONOMICA','BODEGA ECONOMICA(SERVITECA,ESTA.SERVIC.)','BODEGAS DE ALMACENAMIENTO NPH','BODEGAS DE ALMACENAMIENTO PH','CENTRO COMERCIAL GRANDE NPH','CENTRO COMERCIAL GRANDE PH','CENTRO COMERCIAL MEDIANO NPH','CENTRO COMERCIAL MEDIANO PH','CENTRO COMERCIAL PEQUENO NPH','CENTRO COMERCIAL PEQUENO PH','CLUBES PEQUENOS','COMERCIO PUNTUAL NPH O HASTA 3 UNID PH','COMERCIO PUNTUAL PH','CORREDOR COMERCIAL NPH O HASTA 3 UNID PH','CORREDOR COMERCIAL PH','DEPOSITOS DE ALMACENAMIENTO NPH','EDIFICIOS DE PARQUEO NPH','EDIFICIOS DE PARQUEO PH','HOTELES NPH','HOTELES PH','MOTELES, AMOBLADOS, RESIDENCIAS NPH','MOTELES, AMOBLADOS, RESIDENCIAS PH','OFICINA BODEGA Y/O INDUSTRIA PH','OFICINAS EN BODEGAS Y/O INDUSTRIAS','OFICINAS OPERATIVAS','OFICINAS OPERATIVAS(ESTACIONES SERVICIO)','OFICINAS Y CONSULTORIOS NPH','OFICINAS Y CONSULTORIOS PH','PARQUES DE DIVERSION','RESTAURANTES NPH','RESTAURANTES PH','TEATROS Y CINEMAS NPH','TEATROS Y CINEMAS PH', 'PARQUES DE DIVERSION EN P.H.'],\n",
    "    'Dotacional':['AULAS DE CLASE','CEMENTERIOS','CENTROS MEDICOS EN PH','CLINICAS, HOSPITALES, CENTROS MEDICOS','CLUBES MAYOR EXTENSION','COLEGIOS EN PH','COLEGIOS Y UNIVERSIDADES 1 A 3 PISOS','COLEGIOS Y UNIVERSIDADES 4 O MAS PISOS','COLISEOS','CULTO RELIGIOS EN NPH','CULTO RELIGIOSO EN PH','IGLESIA PH','IGLESIAS','INSTALACIONES MILITARES','INSTITUCIONAL PH','INSTITUCIONAL PUNTUAL','MUSEOS','OFICINAS Y CONSULTORIOS (OFICIAL) NPH','OFICINAS Y CONSULTORIOS (OFICIAL) PH','PLAZAS DE MERCADO'],\n",
    "    'Industrial':['INDUSTRIA ARTESANAL','INDUSTRIA GRANDE','INDUSTRIA GRANDE PH','INDUSTRIA MEDIANA','INDUSTRIA MEDIANA PH'],\n",
    "    'Otro':['COCHERAS, MARRANERAS, PORQUERIZAS','ENRAMADAS, COBERTIZOS, CANEYES','ESTABLOS, PESEBRERAS','GALPONES, GALLINEROS','KIOSKOS','LOTE EN PROPIEDAD HORIZONTAL','PISTA AEROPUERTO','SILOS'],\n",
    "    'Residencial':['HABITACIONAL EN PROPIEDAD HORIZONTAL','HABITACIONAL MAYOR O IGUAL A 4 PISOS NPH O 3 PISOS PH','HABITACIONAL MENOR O IGUAL A 3 PISOS NPH','HABITACIONAL MENOR O IGUAL A 3 PISOS PH','MAYOR O IGUAL A 4 PISOS NPH O 3 PISOS PH']\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aee7356-20bc-4563-81c7-abbb6faa4361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usos_licencias_homologados = {\n",
    "    'Residencial':['Vivienda'],\n",
    "    'Comercio y servicios':['Comercio','Oficinas','Servicios'],\n",
    "    'Industrial':['Industria'],\n",
    "    'Otros':['Dotacionales','Institucional','Estacionamientos / Parqueaderos','Otros','Sin informacion','Sin Información','Recreativos']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1344b6b3-b214-449c-a29f-2f293be41958",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "modalidades_homologadas = {\n",
    "    'Obra nueva':['1.Obra Nueva','1.Obra nueva'],\n",
    "    'Ampliación':['2.Ampliación'],\n",
    "    'Modificación':['Modificación'],\n",
    "    'Demolición':['Demolición Total', 'Demolición Parcial', 'Demolición'],\n",
    "    'Reforzamiento Estructural':['Reforzamiento Estructural'],\n",
    "    'Restauración':['Restauración'],\n",
    "    'Cerramiento':['Cerramiento'],\n",
    "    'Adecuación':['Adecuación'],\n",
    "    'Subdivisión':['Subdivisión urbana', 'Subdivisión rural'],\n",
    "    'Propiedad Horizontal':['Propiedad Horizontal'],\n",
    "    'Sin Modalidad':['Sin Modalidad', 'Ninguna', 'Sin Información', 'No Aplica Modalidad'],\n",
    "    'Culminación de Obras':['Culminación de Obras'],\n",
    "    'Reconocimiento':['Reconocimiento'],\n",
    "    'Desarrollo':['Desarrollo','Desarrollo por etapas'],\n",
    "    'Otras':['Restitución','Reloteo', 'Área Bruta Urbanismo', 'Área Útil Urbanismo', 'Saneamiento', 'Reconstrucción'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb28abbd-6511-4654-92b4-0da67178e4e9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ac73dd-41f5-4a1b-be0a-651325c3cada",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def remove_words_from_string(input_str, words_to_remove):\n",
    "    for word in words_to_remove:\n",
    "        input_str = input_str.replace(word, \"\")\n",
    "    return input_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d8e192-2a2a-4cd5-8bb3-601f126c66bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_ph_address(s):\n",
    "    s = re.sub(r'\\b(AP|GJ|IN|LT) \\d+\\b', '', s).strip()\n",
    "    if 'SUR' in s:\n",
    "        s = re.sub(r'\\bSUR\\b', '', s).strip() + ' S'\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc68737d-5152-430c-b406-d1169062cc6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"estado\", StringType()), StructField(\"yinput\", DoubleType()), StructField(\"lotcodigo\", StringType()),\n",
    "                    StructField(\"latitude\", StringType()), StructField(\"diraprox\", StringType()), StructField(\"mancodigo\", StringType()),\n",
    "                    StructField(\"cpocodigo\", StringType()), StructField(\"xinput\", DoubleType()), StructField(\"codloc\", StringType()),\n",
    "                    StructField(\"dirtrad\", StringType()), StructField(\"nomupz\", StringType()), StructField(\"localidad\", StringType()),\n",
    "                    StructField(\"dirinput\", StringType()), StructField(\"codupz\", StringType()), StructField(\"nomseccat\", StringType()),\n",
    "                    StructField(\"tipo_direccion\", StringType()), StructField(\"codseccat\", StringType()), StructField(\"longitude\", StringType()),\n",
    "                ])\n",
    "def georeferenciar(input_str:str):\n",
    "    \"\"\"\n",
    "    Realiza la georreferenciación de una cadena de entrada utilizando una API de geocodificación.\n",
    "\n",
    "    Args:\n",
    "        input_str (str): La cadena a georreferenciar.\n",
    "\n",
    "    Returns:\n",
    "        dict: Un diccionario que contiene los datos de georreferenciación si la operación fue exitosa.\n",
    "              En caso contrario, devuelve un diccionario que indica un estado de falla o un estado de error.\n",
    "\n",
    "    \"\"\"\n",
    "    schema = StructType([StructField(\"estado\", StringType()), StructField(\"yinput\", DoubleType()), StructField(\"lotcodigo\", StringType()),\n",
    "                    StructField(\"latitude\", StringType()), StructField(\"diraprox\", StringType()), StructField(\"mancodigo\", StringType()),\n",
    "                    StructField(\"cpocodigo\", StringType()), StructField(\"xinput\", DoubleType()), StructField(\"codloc\", StringType()),\n",
    "                    StructField(\"dirtrad\", StringType()), StructField(\"nomupz\", StringType()), StructField(\"localidad\", StringType()),\n",
    "                    StructField(\"dirinput\", StringType()), StructField(\"codupz\", StringType()), StructField(\"nomseccat\", StringType()),\n",
    "                    StructField(\"tipo_direccion\", StringType()), StructField(\"codseccat\", StringType()), StructField(\"longitude\", StringType()),\n",
    "                ])\n",
    "\n",
    "    input_str = '' if input_str is None else input_str\n",
    "\n",
    "    direc = unicodedata.normalize(\"NFKD\", input_str).encode(\"ascii\",\"ignore\").decode(\"ascii\").lower()\n",
    "\n",
    "    val = [True for t in direc.replace('.',' ').replace(\"  \",\" \").split() if t in ['via','km','chia','cota','tocancipa','tenjo','madrid','tabio,','cajica,','mosquera','zipaquira']]\n",
    "    \n",
    "    if sum(val)>=1:\n",
    "        result = {'estado': 'outside_bogota','yinput':0,'lotcodigo':'','latitude':'','diraprox':'','mancodigo':'','cpocodigo':'','xinput':0,'codloc':'','dirtrad':'','nomupz':'','localidad':'','dirinput':'','codupz':'','nomseccat':'','tipo_direccion':'','codseccat':'','longitude':''}\n",
    "        return result\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            words_to_remove = [\"costado occ.\", \"esquina\"]\n",
    "            direc = remove_words_from_string(direc, words_to_remove)\n",
    "            direc = clean_ph_address(direc)\n",
    "\n",
    "            pattern = r'av\\.?(?: de la)? esp\\.?|\\bavenida la esperanza\\b|\\besperanza\\b'\n",
    "            replacement_str = \"AC 24\"\n",
    "            output_str = re.sub(pattern, replacement_str, direc)\n",
    "\n",
    "            pattern = r'av\\.?(?: de las)? ame\\.?|\\bavenida de las americas\\b|\\bamericas\\b'\n",
    "            replacement_str = \"AC 9\"\n",
    "            output_str = re.sub(pattern, replacement_str, output_str)\n",
    "\n",
    "            pattern = r'(autop\\.?(\\s)?norte|autonorte|au\\.?(\\s)?norte|aut\\.?(\\s)?norte)'\n",
    "            replacement_str = \"AK 45\"\n",
    "            output_str = re.sub(pattern, replacement_str, output_str)\n",
    "\n",
    "            pattern = r'av\\.?(?: de las)?(?: c\\.?)?\\s*(?:ciudad\\s*)?cali|avenida\\s*cali'\n",
    "            replacement_str = \"AK 86\"\n",
    "            output_str = re.sub(pattern, replacement_str, output_str)\n",
    "\n",
    "            pattern = r'av. nqs|\\bnqs\\b'\n",
    "            replacement_str = \"AK 30\"\n",
    "            output_str = re.sub(pattern, replacement_str, output_str)\n",
    "\n",
    "            pattern = r'av. suba|\\bsuba\\b|tr. suba'\n",
    "            replacement_str = \"AK 69\"\n",
    "            output_str = re.sub(pattern, replacement_str, output_str)\n",
    "\n",
    "            output_str = output_str.replace('cl. av.', 'AC')\n",
    "            output_str = output_str.replace('av. cl.', 'AC')\n",
    "            output_str = output_str.replace('av. cr.', 'AK')\n",
    "\n",
    "            con = False\n",
    "\n",
    "            output_str = output_str.replace('con','#')\n",
    "            output_str = output_str.replace('paralela','#')\n",
    "\n",
    "            parts = output_str.split(\"#\")\n",
    "\n",
    "            if len(parts) > 1:\n",
    "                if '-' not in parts[1]:\n",
    "                    parts[1] = re.sub(r'[a-zA-Z.]', '', parts[1])\n",
    "                    output_str = \"#\".join(parts)\n",
    "\n",
    "            \n",
    "\n",
    "            pattern = r'(\\d+)\\D+(\\w+)'\n",
    "            output_str = re.sub(pattern, lambda x: x.group(1) + ' # ' + x.group(2), output_str)\n",
    "\n",
    "            if '-' not in output_str:\n",
    "                output_str = output_str + ' - 01'\n",
    "\n",
    "\n",
    "            parts = output_str.split(\"#\")\n",
    "\n",
    "            if parts[0].strip() == 'AK 30' and (int(parts[1].split('-')[0].strip())>100):\n",
    "                parts[0] = 'AK 9'\n",
    "                output_str = \"#\".join(parts)\n",
    "\n",
    "            output_str = output_str.replace(\"AK 69\",'Av Suba')\n",
    "\n",
    "            print(output_str)\n",
    "\n",
    "            params = {\n",
    "                'cmd': 'geocodificar',\n",
    "                'apikey': '1c025f92-4520-49c1-90a7-a24b3d9d374c',\n",
    "                'query': output_str\n",
    "            }\n",
    "\n",
    "            response = requests.get(url, params=params)\n",
    "            res = response.json()\n",
    "            success = res['response']['success']\n",
    "            if success:\n",
    "                result = res['response']['data']\n",
    "                return result\n",
    "            else:\n",
    "                result = {'estado': 'failed','yinput':0,'lotcodigo':'','latitude':'','diraprox':'','mancodigo':'','cpocodigo':'','xinput':0,'codloc':'','dirtrad':'','nomupz':'','localidad':'','dirinput':'','codupz':'','nomseccat':'','tipo_direccion':'','codseccat':'','longitude':''}\n",
    "                return result\n",
    "        except:\n",
    "            result = {'estado': 'error_ws','yinput':0,'lotcodigo':'','latitude':'','diraprox':'','mancodigo':'','cpocodigo':'','xinput':0,'codloc':'','dirtrad':'','nomupz':'','localidad':'','dirinput':'','codupz':'','nomseccat':'','tipo_direccion':'','codseccat':'','longitude':''}\n",
    "            \n",
    "            return result\n",
    "        \n",
    "georeferenciar_udf = udf(georeferenciar, returnType=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f8b3c48-58b3-44b6-a8ee-ff6828935694",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "create_point_udf = udf(lambda x,y: Point(x,y).wkt, returnType=StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a56ed5-8a89-44a4-8273-c992319522c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def homologar_uso(uso_catastro:str,usos_agregados=usos_homologados):\n",
    "    '''\n",
    "    Toma el uso de la base de predios de catastro y lo agrega a \n",
    "    categorias mas sencillas de entender para el análisis de datos del OOVS\n",
    "\n",
    "    Args:\n",
    "        input_str (str): Cadena de texto que trae el uso para gregarlo.\n",
    "\n",
    "    Returns:\n",
    "        str: retorna el uso agragado y homologado por el equipo del observatorio\n",
    "    '''\n",
    "    for k,v in usos_agregados.items():\n",
    "        if uso_catastro in v:\n",
    "            return k\n",
    "        elif uso_catastro not in v:\n",
    "            pass\n",
    "homoUso_udf = udf(homologar_uso, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37925cbd-debf-4fc0-bb8d-e29dad8b75a9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def homologar_uso_licencia(uso:str,usos_agregados=usos_licencias_homologados):\n",
    "    '''\n",
    "    Toma el uso de la base de predios de catastro y lo agrega a \n",
    "    categorias mas sencillas de entender para el análisis de datos del OOVS\n",
    "\n",
    "    Args:\n",
    "        input_str (str): Cadena de texto que trae el uso para gregarlo.\n",
    "\n",
    "    Returns:\n",
    "        str: retorna el uso agragado y homologado por el equipo del observatorio\n",
    "    '''\n",
    "    for k,v in usos_agregados.items():\n",
    "        if uso in v:\n",
    "            return k\n",
    "        elif uso not in v:\n",
    "            pass\n",
    "homoUsoLicencia_udf = udf(homologar_uso_licencia, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cd7a7c6-c08f-406d-8a39-3968a8e31018",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def homologar_modalidad(modalidad_licencia:str,modalidades_agregados=modalidades_homologadas):\n",
    "    '''\n",
    "    Toma el uso de la base de predios de catastro y lo agrega a \n",
    "    categorias mas sencillas de entender para el análisis de datos del OOVS\n",
    "\n",
    "    Args:\n",
    "        input_str (str): Cadena de texto que trae el uso para gregarlo.\n",
    "\n",
    "    Returns:\n",
    "        str: retorna el uso agragado y homologado por el equipo del observatorio\n",
    "    '''\n",
    "    for k,v in modalidades_agregados.items():\n",
    "        if modalidad_licencia in v:\n",
    "            return k\n",
    "        elif modalidad_licencia not in v:\n",
    "            pass\n",
    "homoModalidad_udf = udf(homologar_modalidad, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d496a477-c72a-40fc-a3af-3ed7e8e9e8b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def transform_coordinates(x, y):\n",
    "    if x is not None and y is not None and x != 'NULL' and y!='NULL' and x != 0.0 and y!=0.0 and isinstance(x, (float)) and isinstance(y, (float)):\n",
    "        source_crs = pyproj.CRS(\"ESRI:102233\")\n",
    "        target_crs = pyproj.CRS(\"EPSG:4326\")\n",
    "        transformer = pyproj.Transformer.from_crs(source_crs, target_crs, always_xy=True)\n",
    "        new_x, new_y = transformer.transform(x, y)\n",
    "        return new_x, new_y\n",
    "    else:\n",
    "        return None, None\n",
    "\n",
    "coordinate_udf = udf(transform_coordinates, StructType([StructField(\"wgs84_lng\", DoubleType(), False),\n",
    "                                                        StructField(\"wgs84_lat\", DoubleType(), False)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bc5358b-0059-45cc-bff3-8fa7e30c71f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_year_from_path(path, paths_dict):\n",
    "    # Iterate over the dictionary to find the year for the given path\n",
    "    for year, paths in paths_dict.items():\n",
    "        if path in paths:\n",
    "            return year\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b176779-d93d-4678-80aa-5886cbcb5c4e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cargue de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "791dad88-ee5f-443f-8362-4f737a362418",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Catastro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e732ebb-ae55-4439-a4be-d9c0e7b9a9cd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El siguiente recorre una estructura de directorios y buscar archivos específicos relacionados con \"ph\", \"predios\" y \"usos\" para los años determinados en el rango"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a1cfa93-6942-4c84-ab24-de2ffa41e88e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se inicializan listas vacías para almacenar rutas de archivos.\n",
    "ph = list()  # Lista para almacenar rutas de archivos relacionados con 'ph'.\n",
    "pre = list()  # Lista para almacenar rutas de archivos relacionados con 'predios'.\n",
    "uso = list()  # Lista para almacenar rutas de archivos relacionados con 'usos'.\n",
    "dict_anio = dict()  # Diccionario para almacenar rutas de archivos por año.\n",
    "\n",
    "# Se recorre la estructura de directorios a partir de 'startpath'.\n",
    "for root, dirs, files in os.walk(startpath):\n",
    "    # Se crea una lista de periodos (años) desde 2006 hasta 2022.\n",
    "    periodos = [str(a) for a in range(2006,2024)]\n",
    "    \n",
    "    # Si el nombre del directorio actual (root) está en la lista de periodos...\n",
    "    if os.path.basename(root) in periodos:\n",
    "        # Se obtiene el nombre del directorio actual (que representa un año).\n",
    "        carpeta = os.path.basename(root)\n",
    "        \n",
    "        # Se construyen las rutas completas para los archivos esperados en ese directorio/año.\n",
    "        ph_predios = '{}/ph_{}.csv'.format(root, carpeta)\n",
    "        predios = '{}/predios_{}.csv'.format(root, carpeta)\n",
    "        usos = '{}/usos_{}.csv'.format(root, carpeta)\n",
    "        \n",
    "        # Se inicializa una lista interna para almacenar las rutas de archivos encontrados en el directorio actual.\n",
    "        lista_interna = list()\n",
    "        \n",
    "        # Se recorren los archivos en el directorio actual.\n",
    "        for f in files:\n",
    "            # Si el archivo es de 'ph'...\n",
    "            if 'ph_' in f:\n",
    "                lista_interna.append(ph_predios)\n",
    "                ph.append(ph_predios)\n",
    "            # Si el archivo es de 'predios'...\n",
    "            elif 'predios_'in f:\n",
    "                lista_interna.append(predios)\n",
    "                pre.append(predios)\n",
    "            # Si el archivo es de 'usos'...\n",
    "            elif 'usos_'in f:\n",
    "                lista_interna.append(usos)\n",
    "                uso.append(usos)\n",
    "            # Si no coincide con ninguno de los anteriores, no se hace nada.\n",
    "            else:\n",
    "                pass\n",
    "        \n",
    "        # Se añade la lista interna al diccionario, usando el año como clave.\n",
    "        dict_anio[carpeta] = lista_interna\n",
    "        \n",
    "        # Se elimina la lista interna para liberar memoria.\n",
    "        del lista_interna\n",
    "    # Si el directorio actual no es un año de interés, no se hace nada.\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2298db9a-21cb-40fc-a857-c55a60be7886",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se lee un archivo shapefile de lotes y se convierte en un GeometryRDD.\n",
    "lotes_shp = ShapefileReader.readToGeometryRDD(sc, \"dbfs:/FileStore/tables/shp/Lotes_centroide/\")\n",
    "\n",
    "# Se lee un archivo shapefile de isocronas y se convierte en un GeometryRDD.\n",
    "isocrona_shp = ShapefileReader.readToGeometryRDD(sc, \"dbfs:/FileStore/tables/shp/isocronas/\")\n",
    "\n",
    "# Se realiza una transformación del sistema de referencia de coordenadas (CRS) para el GeometryRDD de lotes.\n",
    "lotes_shp.CRSTransform('epsg:4326','epsg:4326')\n",
    "\n",
    "# Se realiza una transformación del CRS para el GeometryRDD de isocronas, similar al anterior.\n",
    "isocrona_shp.CRSTransform('epsg:4326','epsg:4326')\n",
    "\n",
    "# Se convierte el GeometryRDD de lotes en un DataFrame de Spark.\n",
    "lotes_df = Adapter.toDf(lotes_shp, spark)\n",
    "\n",
    "# Se convierte el GeometryRDD de isocronas en un DataFrame de Spark.\n",
    "isocrona_df = Adapter.toDf(isocrona_shp, spark)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73d45780-c275-4b1d-8680-886adb442f7b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cruce de lotes con isocrónas proyecto Metro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4ae480-5a8f-45b3-8ed3-7e4e0dd16d22",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se analiza el objeto 'lotes_shp' para obtener estadísticas básicas y optimizar futuras operaciones.\n",
    "lotes_shp.analyze()\n",
    "\n",
    "# Se realiza una partición espacial del objeto 'lotes_shp' utilizando un árbol KDBTREE con 4 particiones.\n",
    "lotes_shp.spatialPartitioning(GridType.KDBTREE, 4)\n",
    "\n",
    "# Se realiza una partición espacial del objeto 'isocrona_shp' utilizando el particionador del objeto 'lotes_shp'.\n",
    "isocrona_shp.spatialPartitioning(lotes_shp.getPartitioner())\n",
    "\n",
    "# Se establecen variables para determinar si se construirá sobre un RDD particionado espacialmente y si se usará un índice.\n",
    "build_on_spatial_partitioned_rdd = True  # Se establece a TRUE solo si se ejecuta una consulta de unión.\n",
    "using_index = True\n",
    "\n",
    "# Se construye un índice QUADTREE para el objeto 'isocrona_shp'.\n",
    "isocrona_shp.buildIndex(IndexType.QUADTREE, build_on_spatial_partitioned_rdd)\n",
    "\n",
    "# Se realiza una consulta de unión espacial entre 'lotes_shp' e 'isocrona_shp'.\n",
    "lotes_isocrona_join = JoinQuery.SpatialJoinQueryFlat(lotes_shp, isocrona_shp, using_index, True)\n",
    "\n",
    "# Se convierte el resultado de la unión espacial en un GeoDataFrame de Geopandas.\n",
    "lotes_gdf = gpd.GeoDataFrame(\n",
    "    # Se mapea el resultado para extraer geometrías y datos asociados.\n",
    "    lotes_isocrona_join.map(lambda x: [x[1].geom, *x[0].userData.split(\"\\t\"), *x[1].userData.split(\"\\t\")]).collect(),\n",
    "    # Se definen las columnas del GeoDataFrame.\n",
    "    columns=[\"geom\", 'group_inde','Tiempo','layer','Linea','LOTSECT_ID','LOTMANZ_ID','LOTLOTE_ID','LOTZHF_ID','LOTZHG_ID','LOTUNIDAPH','LOTDISTRIT','LOTLSIMBOL','ESTADO_REG','FECHA_REGI','FECHA_DESD','FECHA_HAST','LOTLNUMERO','FRENTE','FONDO'],\n",
    "    # Se establece la columna de geometría.\n",
    "    geometry=\"geom\",\n",
    "    # Se define el sistema de referencia de coordenadas.\n",
    "    crs= \"epsg:4326\"\n",
    ")\n",
    "\n",
    "# Extract coordinates from geom field in a geopandas df\n",
    "lotes_gdf['def_lng'] = lotes_gdf['geom'].apply(lambda p: p.x)\n",
    "lotes_gdf['def_lat'] = lotes_gdf['geom'].apply(lambda p: p.y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0e19246-e556-4a75-a857-813b319c3525",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "lotes_gdf[['geom','Tiempo']].sample(5000).explore('Tiempo', tiles='CartoDB positron')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf28e976-5494-4601-a96f-937b3957c43a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Cruce de lotes con usos e inmuebles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e17943f-7dc2-4b25-b039-9caddaa84b94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Se recorre cada ruta de archivo en la lista 'uso'.\n",
    "for u in uso:\n",
    "    # Se reemplaza la parte \"/dbfs\" en la ruta del archivo con \"dbfs:\" para ajustar la ruta al formato requerido por Spark en ciertos entornos.\n",
    "    path = u.replace(\"/dbfs\",\"dbfs:\")\n",
    "    \n",
    "    # Se lee el archivo CSV en un DataFrame de Spark.\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    year = get_year_from_path(u, dict_anio)\n",
    "    if year:\n",
    "        df = df.withColumn(\"año_vigencia\", lit(year))\n",
    "    else:\n",
    "        df = df.withColumn(\"año_vigencia\", lit(0))\n",
    "    \n",
    "    \n",
    "    df = df.withColumn(\"año_vigencia\", col(\"año_vigencia\").cast(\"int\"))\n",
    "\n",
    "    # Se aplica una función definida por el usuario (UDF) llamada 'homoUso_udf' al DataFrame.\n",
    "    # Esta función parece homologar o transformar la columna \"DESCRIPCION_USO\".\n",
    "    # El resultado se almacena en una nueva columna llamada \"UsoAgregado\".\n",
    "    df = df.withColumn(\"UsoAgregado\", homoUso_udf(df[\"DESCRIPCION_USO\"]))\n",
    "    \n",
    "    # Se modifica el nombre del archivo original, reemplazando '.csv' con '_agregado.csv'.\n",
    "    # Esto sugiere que el archivo resultante contendrá datos \"homologados\" o \"agregados\".\n",
    "    new_name = path.replace('.csv','_agregado.csv')\n",
    "    \n",
    "    # Se escribe el DataFrame modificado de vuelta a un archivo CSV con el nuevo nombre.\n",
    "    # Si un archivo con ese nombre ya existe, se sobrescribirá.\n",
    "    df.write.csv(new_name, header=True, mode=\"overwrite\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e478f5d6-bead-4dfc-9e43-63cb4609c287",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Generación de dataframe consolidado usos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca7cd5cb-0bf8-4f57-9655-ec7ba4941094",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializamos un DataFrame vacío llamado usos_agregados_df.\n",
    "usos_agregados_df = None\n",
    "\n",
    "# Iteramos sobre cada elemento en la lista 'uso', que parece contener rutas a archivos CSV.\n",
    "for u in uso:\n",
    "    # Reemplazamos \"/dbfs\" con \"dbfs:\" y cambiamos la extensión '.csv' por '_agregado.csv' en la ruta del archivo.\n",
    "    path = u.replace(\"/dbfs\",\"dbfs:\")\n",
    "    path = path.replace('.csv','_agregado.csv')\n",
    "    \n",
    "    # Leemos el archivo CSV desde la ruta modificada 'path' usando Spark y lo almacenamos en el DataFrame 'df'.\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    \n",
    "    # Obtenemos el año de la ruta del archivo usando la función 'get_year_from_path' y el diccionario 'dict_anio'.\n",
    "    year = get_year_from_path(u, dict_anio)\n",
    "    \n",
    "    # Si se encuentra un año en la ruta del archivo, agregamos una nueva columna 'año_vigencia' con ese año al DataFrame 'df'.\n",
    "    # De lo contrario, agregamos la columna 'año_vigencia' con el valor 0.\n",
    "    if year:\n",
    "        df = df.withColumn(\"año_vigencia\", lit(year))\n",
    "    else:\n",
    "        df = df.withColumn(\"año_vigencia\", lit(0))\n",
    "\n",
    "    df = df.withColumn(\"año_vigencia\", col(\"año_vigencia\").cast(\"int\"))\n",
    "    \n",
    "    # Si el DataFrame 'usos_agregados_df' está vacío (es decir, es la primera iteración), lo inicializamos con 'df'.\n",
    "    # De lo contrario, unimos 'df' con 'usos_agregados_df' usando la operación 'union'.\n",
    "    if usos_agregados_df is None:\n",
    "        usos_agregados_df = df\n",
    "    else:\n",
    "        usos_agregados_df = usos_agregados_df.union(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d4690a-8bdd-4ae5-b5e1-805cafd9dba4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Conteo de usos por lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d76f7b5-5aa6-4f86-8c95-dd2bb8c094c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupamos el DataFrame 'usos_agregados_df' por las columnas 'LOTLOTE_ID' y 'año_vigencia'.\n",
    "# Luego, pivotamos el DataFrame usando la columna 'UsoAgregado' para transformar sus valores únicos en columnas individuales.\n",
    "# Posteriormente, contamos las ocurrencias de cada combinación de 'LOTLOTE_ID', 'año_vigencia' y 'UsoAgregado'.\n",
    "# El resultado es un DataFrame que tiene una columna para cada valor único de 'UsoAgregado' y celdas que indican el conteo de cada combinación.\n",
    "conteo_usos = usos_agregados_df.groupBy([\"LOTLOTE_ID\",\"año_vigencia\"]).pivot(\"UsoAgregado\").count().fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ccc7cf4-87c1-48c3-941c-828f851cb685",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Generación de dataframe consolidado predios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f938c3dc-f4e7-4a79-a692-9cc15cbd81f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializamos un DataFrame vacío llamado predios_agregados_df.\n",
    "predios_agregados_df = None\n",
    "\n",
    "# Iteramos sobre cada elemento en la lista 'pre', que parece contener rutas a archivos CSV.\n",
    "for p in pre:\n",
    "    # Reemplazamos \"/dbfs\" con \"dbfs:\" en la ruta del archivo.\n",
    "    path = p.replace(\"/dbfs\",\"dbfs:\")\n",
    "    print(path)\n",
    "    \n",
    "    # Leemos el archivo CSV desde la ruta 'path' usando Spark y lo almacenamos en el DataFrame 'df'.\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    \n",
    "    # Obtenemos el año de la ruta del archivo usando la función 'get_year_from_path' y el diccionario 'dict_anio'.\n",
    "    # Nota: Parece haber un error en esta línea. Debería ser 'get_year_from_path(p, dict_anio)' en lugar de 'get_year_from_path(u, dict_anio)'.\n",
    "    year = get_year_from_path(p, dict_anio)\n",
    "    \n",
    "    # Si se encuentra un año en la ruta del archivo, agregamos una nueva columna 'año_vigencia' con ese año al DataFrame 'df'.\n",
    "    # De lo contrario, agregamos la columna 'año_vigencia' con el valor 0.\n",
    "    if year:\n",
    "        df = df.withColumn(\"año_vigencia\", lit(year))\n",
    "    else:\n",
    "        df = df.withColumn(\"año_vigencia\", lit(0))\n",
    "\n",
    "    df = df.withColumn(\"año_vigencia\", col(\"año_vigencia\").cast(\"int\"))\n",
    "    \n",
    "    # Si el DataFrame 'predios_agregados_df' está vacío (es decir, es la primera iteración), lo inicializamos con 'df'.\n",
    "    # De lo contrario, unimos 'df' con 'predios_agregados_df' usando la operación 'union'.\n",
    "    if predios_agregados_df is None:\n",
    "        predios_agregados_df = df\n",
    "    else:\n",
    "        predios_agregados_df = predios_agregados_df.union(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "565092aa-5e1a-4032-94a3-186d0c3b4148",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Suma de área terreno, área construida, valor avalúo y conteo de CHIPs por lote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9c19eb2-e6f7-4ec2-a390-8010a961df04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupamos el DataFrame 'predios_agregados_df' por las columnas 'LOTLOTE_ID' y 'año_vigencia'.\n",
    "# Luego, aplicamos varias funciones de agregación:\n",
    "# - Sumamos los valores de la columna 'AREA_TERRENO'.\n",
    "# - Sumamos los valores de la columna 'AREA_CONSTRUIDA'.\n",
    "# - Sumamos los valores de la columna 'VALOR_AVALUO'.\n",
    "# - Contamos el número de valores en la columna 'CHIP'.\n",
    "# El resultado es un DataFrame que tiene una columna para cada función de agregación aplicada.\n",
    "conteo_predios = predios_agregados_df.groupBy([\"LOTLOTE_ID\",\"año_vigencia\"]).agg({\"AREA_TERRENO\":\"sum\", \"AREA_CONSTRUIDA\":\"sum\", \"VALOR_AVALUO\":\"sum\", \"CHIP\":\"count\"}).fillna(0)\n",
    "\n",
    "# Renombramos las columnas del DataFrame 'conteo_predios' para que tengan nombres más descriptivos y fáciles de entender.\n",
    "# - \"count(CHIP)\" se renombra a \"NumPredios\".\n",
    "# - \"sum(VALOR_AVALUO)\" se renombra a \"ValorAvaluo\".\n",
    "# - \"sum(AREA_CONSTRUIDA)\" se renombra a \"AC\".\n",
    "# - \"sum(AREA_TERRENO)\" se renombra a \"AT\".\n",
    "conteo_predios = conteo_predios.withColumnRenamed(\"count(CHIP)\",\"NumPredios\") \\\n",
    "    .withColumnRenamed(\"sum(VALOR_AVALUO)\",\"ValorAvaluo\") \\\n",
    "    .withColumnRenamed(\"sum(AREA_CONSTRUIDA)\",\"AC\") \\\n",
    "    .withColumnRenamed(\"sum(AREA_TERRENO)\",\"AT\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c396003-6703-4419-a607-9f6af0fb1cda",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Generación de dataframe consolidado propiedad horizontal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdeb90e6-60f7-4ff0-9f55-c307c403abf7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Inicializamos un DataFrame vacío llamado ph_agregados_df.\n",
    "ph_agregados_df = None\n",
    "\n",
    "# Iteramos sobre cada elemento en la lista 'ph', que parece contener rutas a archivos CSV.\n",
    "for p in ph:\n",
    "    # Reemplazamos \"/dbfs\" con \"dbfs:\" en la ruta del archivo.\n",
    "    path = p.replace(\"/dbfs\",\"dbfs:\")\n",
    "    \n",
    "    # Leemos el archivo CSV desde la ruta 'path' usando Spark y lo almacenamos en el DataFrame 'df'.\n",
    "    df = spark.read.format(\"csv\").option(\"header\", \"true\").load(path)\n",
    "    \n",
    "    # Obtenemos el año de la ruta del archivo usando la función 'get_year_from_path' y el diccionario 'dict_anio'.\n",
    "    # Nota: Parece haber un error en esta línea. Debería ser 'get_year_from_path(p, dict_anio)' en lugar de 'get_year_from_path(u, dict_anio)'.\n",
    "    year = get_year_from_path(p, dict_anio)\n",
    "    print(p)\n",
    "    print(year)\n",
    "    \n",
    "    # Si se encuentra un año en la ruta del archivo, agregamos una nueva columna 'año_vigencia' con ese año al DataFrame 'df'.\n",
    "    # De lo contrario, agregamos la columna 'año_vigencia' con el valor 0.\n",
    "    if year:\n",
    "        df = df.withColumn(\"año_vigencia\", lit(year))\n",
    "    else:\n",
    "        df = df.withColumn(\"año_vigencia\", lit(0))\n",
    "\n",
    "    df = df.withColumn(\"año_vigencia\", col(\"año_vigencia\").cast(\"int\"))\n",
    "    \n",
    "    # Si el DataFrame 'ph_agregados_df' está vacío (es decir, es la primera iteración), lo inicializamos con 'df'.\n",
    "    # De lo contrario, unimos 'df' con 'ph_agregados_df' usando la operación 'union'.\n",
    "    if ph_agregados_df is None:\n",
    "        ph_agregados_df = df\n",
    "    else:\n",
    "        ph_agregados_df = ph_agregados_df.union(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76c807d5-7ef0-443e-b35e-9bebf22d901a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Seleccionamos solo las columnas \"LOTLOTE_ID\", \"año_vigencia\" y \"NUMERO_UNIDADES_PH\" del DataFrame 'ph_agregados_df'.\n",
    "# Esto reduce el DataFrame a solo estas tres columnas, descartando cualquier otra columna que pueda existir en 'ph_agregados_df'.\n",
    "ph_agregados_df = ph_agregados_df.select(\"LOTLOTE_ID\",\"año_vigencia\",\"NUMERO_UNIDADES_PH\")\n",
    "\n",
    "# Renombramos la columna \"NUMERO_UNIDADES_PH\" a \"Unidades_En_PH\" en el DataFrame 'ph_agregados_df'.\n",
    "# Esto se hace para tener un nombre de columna más descriptivo y fácil de entender.\n",
    "ph_agregados_df = ph_agregados_df.withColumnRenamed(\"NUMERO_UNIDADES_PH\",\"Unidades_En_PH\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "047ba941-145d-48a5-a719-06ea53a821bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "campos_lotes = ['LOTLOTE_ID','Tiempo','layer','Linea','def_lat', 'def_lng']\n",
    "lotes_df = pd.DataFrame(lotes_gdf)[campos_lotes].drop_duplicates()\n",
    "conteo_predios_df = conteo_predios.toPandas().drop_duplicates()\n",
    "conteo_usos_df = conteo_usos.toPandas().drop_duplicates()\n",
    "ph_agregados_df = ph_agregados_df.toPandas().drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aed1545e-ce40-452d-bf5a-da1785bbeaf9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Unión de predios con conteos de usos por lote y con conteo de unidades ph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "378395e4-cc33-4f87-adf3-961c9171f3fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_lotes_conteo_predios = pd.merge(left=lotes_df, right=conteo_predios_df, on=['LOTLOTE_ID'], how='left')\n",
    "\n",
    "union_predios_usos_df = pd.merge(left=union_lotes_conteo_predios, right=conteo_usos_df, on=['LOTLOTE_ID','año_vigencia'], how='left')\n",
    "\n",
    "union_catastro_df = pd.merge(left=union_predios_usos_df, right=ph_agregados_df, on=['LOTLOTE_ID','año_vigencia'], how='left').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2cb1a618-505e-4d1c-b6cb-665969db96c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_catastro_df.columns = ['LOTLOTE_ID','Tiempo','Estacion','Linea','DefLat','DefLng','AnioVigencia','NumPredios','ValorAvaluo','AC','AT','AsociarAlUsoPrincipal','ComercioYServicios','Dotacional','Industrial','Otro','Residencial','UnidadesEnPH']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0c18361-ebfd-4803-a853-bacd390f8f56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Pandas a un DataFrame de PySpark\n",
    "# union_catastro_df.iteritems = union_catastro_df.items\n",
    "union_catastro_spark_df = spark.createDataFrame(union_catastro_df)\n",
    "union_catastro_spark_df.write.csv('/FileStore/tables/Catastro/union_catastro_2006_2023.csv', header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08b9e7c7-906d-4918-8a02-7f6fe905873e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_catastro_spark_df.createOrReplaceTempView(\"union_catastro_spark_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d36a980e-7203-4caf-aaa3-c61aae9677f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE OR REPLACE TABLE lotes_catastro_final AS (\n",
    "  Select \n",
    "  u.*, e.ESoEstrato Estrato, case when AC <> 0 then ValorAvaluo/AC else 0 end as VC, AC/AT as IC\n",
    "  from union_catastro_spark_df u\n",
    "  left join estrato_socioeconomico e on u.LOTLOTE_ID = e.ESoCLote);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f18fe97-c278-4490-8f46-3f0c40f100bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM lotes_catastro where AnioVigencia = 2022\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05c0edaa-7060-42fd-aa82-1a64592ac08d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7d63eb1-4f00-4016-bbbc-0eed3ccd4558",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.sql(\"SELECT * FROM lotes_catastro_final where AnioVigencia = 2023\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0640e78e-790a-40f8-999f-9b36947f07dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6964478e-cdee-4aa5-a8d4-4150685f5a8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# First, sort the data by revenue and assign percent rank\n",
    "df_with_rank = df.withColumn(\"percent_rank\", F.percent_rank().over(Window.orderBy(\"VC\")))\n",
    "\n",
    "df_with_rank = df_with_rank.where(df.VC <= 20000000)\n",
    "\n",
    "# Function to determine the division based on the percent rank\n",
    "def get_division(rank):\n",
    "    if 0 < rank <= 1:\n",
    "        for i in range(1, 31):  # for 30 divisions\n",
    "            if rank <= i/30:\n",
    "                # Use ordinal function to get \"1st\", \"2nd\", \"3rd\", etc.\n",
    "                ordinal = lambda n: \"%d%s\" % (n, \"tsnrhtdd\"[((n//10%10!=1)*(n%10<4)*n%10)::4])\n",
    "                return f\"{ordinal(i)} Division\"\n",
    "    else:\n",
    "        return \"Invalid Rank\"\n",
    "\n",
    "\n",
    "# UDF to compute the division\n",
    "get_division_udf = F.udf(get_division, \"string\")\n",
    "\n",
    "# Applying the division function to the percent_rank column and adding the division information to the DataFrame\n",
    "df_with_division = df_with_rank.withColumn(\"Division\", get_division_udf(\"percent_rank\"))\n",
    "\n",
    "# Grouping the data by division and computing the range (minimum and maximum revenue) for each division\n",
    "division_ranges = df_with_division.groupBy(\"Division\").agg(\n",
    "    F.min(\"VC\").alias(\"RangeStart\"),\n",
    "    F.max(\"VC\").alias(\"RangeEnd\")\n",
    ")\n",
    "\n",
    "# Displaying the result\n",
    "division_ranges.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bff939c-a113-4c20-aeb5-1ddd52d69430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session (assuming you haven't already)\n",
    "spark = SparkSession.builder.appName(\"KMeansBreaks\").getOrCreate()\n",
    "\n",
    "sampled_df = df.where(df.VC <= 50000000)\n",
    "\n",
    "# Convert to vector for k-means\n",
    "vec_assembler = VectorAssembler(inputCols=[\"VC\"], outputCol=\"features\")\n",
    "vec_df = vec_assembler.transform(sampled_df)\n",
    "\n",
    "# Apply k-means clustering\n",
    "k = 7  # Number of clusters, adjust as needed\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(vec_df)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "# Extract cluster centers and sort them\n",
    "sorted_centers = sorted([center[0] for center in centers])\n",
    "\n",
    "# Determine break points\n",
    "breaks = [(sorted_centers[i] + sorted_centers[i+1]) / 2 for i in range(len(sorted_centers) - 1)]\n",
    "\n",
    "print(breaks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10a079c8-3cea-4d06-bb9e-a3110ee7712d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import numpy as np\n",
    "import jenkspy\n",
    "\n",
    "# Step 1: Filter and clean the data\n",
    "filtered_df = df.filter(\n",
    "    (col(\"VC\").isNotNull()) &\n",
    "    (col(\"VC\") != np.inf) &\n",
    "    (col(\"VC\") != -np.inf) &\n",
    "    (col(\"VC\") <= 30000000)\n",
    ")\n",
    "\n",
    "# Step 2: Sampling 5% of the data\n",
    "sampled_data = filtered_df.sample(False, 0.05).select(\"VC\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Compute the Jenks breaks using sampled data\n",
    "breaks = jenkspy.jenks_breaks(sampled_data, n_classes=6)\n",
    "\n",
    "# Print the Jenks breaks table\n",
    "for i, b in enumerate(breaks):\n",
    "    print(f\"Break {i + 1}: {b}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5e333fe-477a-484c-b53f-39e7b35771a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from h3 import h3\n",
    "# Define a UDF to convert lat/lng to H3 index\n",
    "def lat_lng_to_h3(lat, lng, resolution=9):\n",
    "    return h3.geo_to_h3(lat, lng, resolution)\n",
    "\n",
    "h3_udf = udf(lat_lng_to_h3)\n",
    "\n",
    "# Apply the UDF to the DataFrame\n",
    "df_with_h3 = df.withColumn(\"h3_index\", h3_udf(df[\"Deflat\"], df[\"DefLng\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "223a8567-d6d6-43f8-b0c1-9a94e7265bbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result = df_with_h3.groupBy([\"h3_index\",'Linea','Estacion','AnioVigencia']).agg(F.avg(\"VC\").alias(\"avg_vc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d4195d7-6cc5-4b1a-b19d-ae03b2a577b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def h3_to_polygon(h3_index):\n",
    "    geo_boundary = h3.h3_to_geo_boundary(h3_index)\n",
    "    # Convert to a list of [lng, lat] coordinates\n",
    "    return [[coord[1], coord[0]] for coord in geo_boundary]\n",
    "\n",
    "h3_to_polygon_udf = udf(h3_to_polygon)\n",
    "\n",
    "df_with_polygons = result.withColumn(\"polygon\", h3_to_polygon_udf(result[\"h3_index\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5fcd6cb-44c5-445a-aa0c-28bf2f84171b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_with_polygons.createOrReplaceTempView(\"df_with_polygons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d727f36-0eb5-4da6-bcc0-7f6cab389e89",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "drop table agregado_espacial_lotes_catastro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "924a6b89-b1a0-4d84-a21c-98721084e0c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE OR REPLACE TABLE agregado_espacial_lotes_catastro AS (\n",
    "  Select \n",
    "  h3_index, Linea,Estacion,AnioVigencia, avg_vc, polygon as contour\n",
    "  from df_with_polygons );\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4693969-3eef-40c2-bf52-6015825ab99f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from agregado_espacial_lotes_catastro;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c249b619-3805-473b-8165-2bb9782e16ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geojson_features = []\n",
    "\n",
    "for row in df_with_polygons.collect():\n",
    "    feature = {\n",
    "        \"type\": \"Feature\",\n",
    "        \"geometry\": {\n",
    "            \"type\": \"Polygon\",\n",
    "            \"coordinates\": [row[\"polygon\"]]\n",
    "        },\n",
    "        \"properties\": {\n",
    "            \"h3_index\": row[\"h3_index\"],\n",
    "            \"avg_vc\": row[\"avg_vc\"]\n",
    "        }\n",
    "    }\n",
    "    geojson_features.append(feature)\n",
    "\n",
    "geojson_data = {\n",
    "    \"type\": \"FeatureCollection\",\n",
    "    \"features\": geojson_features\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207a9ead-a8b8-4405-88ec-8855c8e56624",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "geojson_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "585c4689-5f99-49a5-b504-fe51a3ed8007",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Predios 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c6720c-391f-45c8-b57c-118726355e0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_catastro_2023 = spark.read.csv('/FileStore/tables/Catastro/2023/predios_2023.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace88217-d9ec-4159-9786-c2f05ca9f8dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_catastro_2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "950eefc3-0632-4225-972f-d5de82b26ebc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_catastro_2023 = df_catastro_2023.withColumn('geocode_result', struct(georeferenciar_udf(col('DIRECCION_REAL')).alias('geocoding')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7bf88d1-bb5e-4ac5-8b48-424a8136ef16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_catastro_2023 = df_catastro_2023.withColumn('estado', col('geocode_result.geocoding.estado'))\n",
    "df_catastro_2023 = df_catastro_2023.withColumn('tipo_direccion', col('geocode_result.geocoding.tipo_direccion'))\n",
    "df_catastro_2023 = df_catastro_2023.withColumn('diraprox', col('geocode_result.geocoding.diraprox'))\n",
    "df_catastro_2023 = df_catastro_2023.withColumn('lat', col('geocode_result.geocoding.yinput'))\n",
    "df_catastro_2023 = df_catastro_2023.withColumn('lng', col('geocode_result.geocoding.xinput'))\n",
    "df_catastro_2023 = df_catastro_2023.withColumn('lotcodigo', col('geocode_result.geocoding.lotcodigo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d8a2f2-e4a9-4ba1-9acb-0113d9c30042",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_catastro_2023 = df_catastro_2023.drop('geocode_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad662e1-4b24-481f-a487-d543b4f02612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_catastro_2023.write.csv('/FileStore/tables/Catastro/2023/predios_2023_georef.csv', header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bef44d67-90c7-4cd1-b8d8-6e7984cff71b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertimos el DataFrame de Spark (df_licencias) a un DataFrame de Pandas (df_licencias_pd).\n",
    "df_catastro_2023 = df_catastro_2023.toPandas()\n",
    "\n",
    "# Filtramos el DataFrame de Pandas para mantener solo las filas donde 'def_lat' no es nulo.\n",
    "df_catastro_2023 = df_catastro_2023[df_catastro_2023['lat'].notnull()]\n",
    "\n",
    "# Filtramos el DataFrame de Pandas para mantener solo las filas donde 'def_lng' no es nulo.\n",
    "df_catastro_2023 = df_catastro_2023[df_licencias_pd['lng'].notnull()]\n",
    "\n",
    "# Creamos una nueva columna 'geometry' en el DataFrame de Pandas.\n",
    "# Esta columna contiene objetos Point (puntos geográficos) creados a partir de las columnas 'def_lng' y 'def_lat'.\n",
    "df_catastro_2023['geometry'] = df_catastro_2023.apply(lambda row: Point(row['lng'], row['lat']), axis=1)\n",
    "\n",
    "# Convertimos el DataFrame de Pandas (df_licencias_pd) a un GeoDataFrame (licencias_gdf) usando geopandas.\n",
    "# Especificamos la columna 'geometry' como la columna que contiene las geometrías y definimos el sistema de referencia de coordenadas (CRS) como \"EPSG:4326\".\n",
    "gdf_catastro_2023 = gpd.GeoDataFrame(df_catastro_2023, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "408ec8ce-75eb-493c-b288-6105cbee18a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Se lee un archivo shapefile de isocronas y se convierte en un GeometryRDD.\n",
    "isocrona_shp = ShapefileReader.readToGeometryRDD(sc, \"dbfs:/FileStore/tables/shp/isocronas/\")\n",
    "\n",
    "# Se realiza una transformación del CRS para el GeometryRDD de isocronas, similar al anterior.\n",
    "isocrona_shp.CRSTransform('epsg:4326','epsg:4326')\n",
    "\n",
    "# Se convierte el GeometryRDD de isocronas en un DataFrame de Spark.\n",
    "isocrona_df = Adapter.toDf(isocrona_shp, spark)\n",
    "\n",
    "# Convertimos el DataFrame de Spark (isocrona_df) a un DataFrame de Pandas (isocronas_pd).\n",
    "isocrona_df = isocrona_df.withColumn(\"geometry\", col(\"geometry\").cast(\"string\"))\n",
    "isocronas_pd = isocrona_df.toPandas()\n",
    "isocronas_pd['geometry'] = isocronas_pd['geometry'].apply(wkt.loads)\n",
    "\n",
    "# Convertimos el DataFrame de Pandas (isocronas_pd) a un GeoDataFrame (isocronas_gdf) usando geopandas.\n",
    "# Especificamos la columna 'geometry' como la columna que contiene las geometrías y definimos el sistema de referencia de coordenadas (CRS) como \"EPSG:4326\".\n",
    "isocronas_gdf = gpd.GeoDataFrame(isocronas_pd, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a0968f-1d02-4498-a80d-99934c3e8c87",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Realizamos una operación de join espacial entre los GeoDataFrames licencias_gdf e isocronas_gdf.\n",
    "# El join se realiza con base en la relación espacial \"within\", es decir, se unen las filas de licencias_gdf que están dentro de las geometrías de isocronas_gdf.\n",
    "# El resultado es un nuevo GeoDataFrame (licencias_gdf_iso) que contiene solo las licencias que están dentro de las isócronas.\n",
    "gdf_catastro_2023_iso = gpd.sjoin(gdf_catastro_2023, isocronas_gdf, how=\"inner\", op=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e53429c0-f2f1-4bf2-91d9-67f1e6b40361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gdf_catastro_2023_iso['wkt'] = pd.Series(\n",
    "        map(lambda geom: str(geom.to_wkt()), gdf_catastro_2023_iso['geometry']),\n",
    "        index=gdf_catastro_2023_iso.index, dtype='string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad5e7bd3-9021-4ca2-a89b-7ef5ddb962b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "catastro_2023_iso_spark_df = spark.createDataFrame(gdf_catastro_2023_iso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d2a3a84-6e79-4b9c-82ab-1bd0865c4651",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "",
       "errorTraceType": null,
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "catastro_2023_iso_spark_df.write.csv('/FileStore/tables/Catastro/2023/predios_2023_georef_iso.csv', header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59c785dc-6cc5-4226-9a9b-ed43f867b936",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "576c5eeb-4ad0-4947-b065-e5b8c7a9f9db",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5b6a7746-4ce0-4f3f-8285-b1187a6fd806",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30215f9e-ee85-4478-834d-051646b0d4bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Licencias (SDP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ab09003-1609-49b7-825f-8c5c4997b51a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definimos la ubicación del archivo que queremos leer.\n",
    "file_location = \"/FileStore/tables/SDP/sdp_licencias_2008_2023.csv\"\n",
    "\n",
    "# Especificamos el tipo de archivo. En este caso, es un archivo CSV.\n",
    "file_type = \"csv\"\n",
    "\n",
    "# Definimos algunas opciones para la lectura del archivo:\n",
    "# - 'infer_schema': Si es \"true\", Spark intentará inferir automáticamente el esquema (tipos de datos) de las columnas.\n",
    "# - 'first_row_is_header': Si es \"true\", Spark tratará la primera fila del archivo CSV como encabezados (nombres de columnas).\n",
    "# - 'delimiter': Especifica el delimitador utilizado en el archivo CSV. En este caso, es una coma.\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# Usamos el método 'read' de Spark para leer el archivo CSV con las opciones especificadas.\n",
    "# El resultado es un DataFrame llamado 'df_licencias' que contiene los datos del archivo CSV.\n",
    "df_licencias = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42333904-6c94-4aef-b6fb-6cbb57109b60",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Georeferenciación de licencias SDP a partir de dirección catastral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a07843f-73b0-4913-ab6f-7b529786ac6a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_licencias = df_licencias.withColumn('geocode_result', struct(georeferenciar_udf(col('Dirección')).alias('geocoding')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c385f11-c4fc-40dc-86f2-50fcb42bee8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_licencias = df_licencias.withColumn('estado', col('geocode_result.geocoding.estado'))\n",
    "# df_licencias = df_licencias.withColumn('tipo_direccion', col('geocode_result.geocoding.tipo_direccion'))\n",
    "# df_licencias = df_licencias.withColumn('diraprox', col('geocode_result.geocoding.diraprox'))\n",
    "# df_licencias = df_licencias.withColumn('lat', col('geocode_result.geocoding.yinput'))\n",
    "# df_licencias = df_licencias.withColumn('lng', col('geocode_result.geocoding.xinput'))\n",
    "# df_licencias = df_licencias.withColumn('lotcodigo', col('geocode_result.geocoding.lotcodigo'))\n",
    "# df_licencias = df_licencias.withColumn('eq_lot_georef', col('lotcodigo').cast(LongType()) == col('Código lote').cast(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a9fd6d3-14dd-4512-806c-5746d73ebd94",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_licencias = df_licencias.drop('geocode_result')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d686bf97-84b7-4683-acbc-052cc323aaa6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# df_licencias.write.csv('/FileStore/tables/SDP/sdp_licencias_2008_2023_georef.csv', header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "349f1ea5-c93a-445a-80ed-99cec16afea3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_licencias = spark.read.csv('/FileStore/tables/SDP/sdp_licencias_2008_2023_georef.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d937ddd-d8e2-4693-b0f6-5cc535e13f3c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agregamos una nueva columna \"new_coords\" al DataFrame df_licencias.\n",
    "# Esta columna se genera utilizando la función definida por el usuario 'coordinate_udf',\n",
    "# que toma como entrada las columnas \"Longitud\" y \"Latitud\".\n",
    "df_licencias = df_licencias.withColumn(\"new_coords\", coordinate_udf(\"Longitud\", \"Latitud\"))\n",
    "\n",
    "# Seleccionamos todas las columnas del DataFrame original, y adicionalmente extraemos\n",
    "# los campos 'wgs84_lng' y 'wgs84_lat' de la columna \"new_coords\".\n",
    "# Posteriormente, eliminamos la columna \"new_coords\" para mantener solo los campos extraídos.\n",
    "df_licencias = df_licencias.select(\"*\", \"new_coords.wgs84_lng\", \"new_coords.wgs84_lat\").drop(\"new_coords\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6ffd2fb-8e26-4e4b-be36-074e3311cdf4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Actualizamos el DataFrame df_licencias agregando una nueva columna \"def_lng\".\n",
    "# Si \"wgs84_lng\" no es nulo y es diferente de 0, se usa el valor de \"wgs84_lng\".\n",
    "# En caso contrario, se utiliza el valor de la columna \"lng\".\n",
    "df_licencias = df_licencias.withColumn(\n",
    "    \"def_lng\",\n",
    "    when(\n",
    "        (col(\"wgs84_lng\").isNotNull()) & \n",
    "        (col(\"wgs84_lng\") != 0), \n",
    "        col(\"wgs84_lng\")\n",
    "    ).otherwise(col(\"lng\"))\n",
    ")\n",
    "\n",
    "# Similar al proceso anterior, actualizamos el DataFrame df_licencias agregando una nueva columna \"def_lat\".\n",
    "# Si \"wgs84_lat\" no es nulo y es diferente de 0, se usa el valor de \"wgs84_lat\".\n",
    "# En caso contrario, se utiliza el valor de la columna \"lat\".\n",
    "df_licencias = df_licencias.withColumn(\n",
    "    \"def_lat\",\n",
    "    when(\n",
    "        (col(\"wgs84_lat\").isNotNull()) & \n",
    "        (col(\"wgs84_lat\") != 0), \n",
    "        col(\"wgs84_lat\")\n",
    "    ).otherwise(col(\"lat\"))\n",
    ")\n",
    "\n",
    "# Definimos una lista de columnas que queremos eliminar del DataFrame.\n",
    "columns_to_drop = [\"Latitud\", \"Longitud\", \"wgs84_lng\", \"wgs84_lat\", \"lat\", \"lng\", \"eq_lot_georef\"]\n",
    "\n",
    "# Eliminamos las columnas especificadas en la lista 'columns_to_drop' del DataFrame df_licencias.\n",
    "df_licencias = df_licencias.drop(*columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7054f211-765b-4ed6-89e9-c9021b1da2e4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Actualizamos el DataFrame df_licencias agregando una nueva columna \"ModalidadAgregada\".\n",
    "# Esta columna se genera utilizando la función definida por el usuario 'homoModalidad_udf',\n",
    "# que toma como entrada la columna \"Modalidad\" del DataFrame.\n",
    "df_licencias = df_licencias.withColumn(\"ModalidadAgregada\", homoModalidad_udf(df_licencias[\"Modalidad\"]))\n",
    "\n",
    "# Eliminamos la columna \"Modalidad\" del DataFrame df_licencias ya que ha sido reemplazada por \"ModalidadAgregada\".\n",
    "df_licencias = df_licencias.drop(\"Modalidad\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c9ef1d-251c-4ee8-8f59-eff42f6d96f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Actualizamos el DataFrame df_licencias agregando una nueva columna \"UsoAgregado\".\n",
    "# Esta columna se genera utilizando la función definida por el usuario 'homoUsoLicencia_udf',\n",
    "# que toma como entrada la columna \"Uso\" del DataFrame.\n",
    "df_licencias = df_licencias.withColumn(\"UsoAgregado\", homoUsoLicencia_udf(df_licencias[\"Uso\"]))\n",
    "\n",
    "# Eliminamos la columna \"Modalidad\" del DataFrame df_licencias ya que ha sido reemplazada por \"ModalidadAgregada\".\n",
    "df_licencias = df_licencias.drop(\"Uso\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c94951e9-182d-4bb1-ac03-ad3d394f3aa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definimos una lista de tipos de trámites que queremos filtrar en el DataFrame.\n",
    "tipo_tramite = ['Licencia de Construcción','Reconocimiento de la existencia de una construcción','Licencia de Urbanización','Licencia de Urbanismo y construcción','Propiedad Horizontal','Licencia de Subdivisión','Revalidación licencia de construcción','Autorización para el movimiento de tierras','Sin informacion','Modificación','Ajuste de cotas de áreas','Copia Certificada de planos','Sin Tipo de Trámite','Inicial','Modificación de planos urbanísticos']\n",
    "tipo_tramite = ['Licencia de Construcción']\n",
    "\n",
    "# Filtramos el DataFrame df_licencias para mantener solo las filas donde 'Tipo Trámite' esté en la lista tipo_tramite.\n",
    "df_licencias = df_licencias.filter(col('Tipo Trámite').isin(tipo_tramite))\n",
    "\n",
    "objeto_tramite = ['Inicial','Modificación']\n",
    "\n",
    "df_licencias = df_licencias.filter(col('Objeto trámite').isin(objeto_tramite))\n",
    "\n",
    "# Definimos una lista de modalidades que queremos filtrar en el DataFrame.\n",
    "modalidades = ['Obra nueva','Ampliación','Modificación','Demolición','Reforzamiento Estructural','Restauración','Cerramiento','Adecuación','Subdivisión','Propiedad Horizontal','Sin Modalidad','Culminación de Obras','Reconocimiento','Desararollo','Otras']\n",
    "modalidades = ['Obra nueva']\n",
    "\n",
    "# Filtramos el DataFrame df_licencias para mantener solo las filas donde 'ModalidadAgregada' esté en la lista modalidades.\n",
    "df_licencias = df_licencias.filter(col('ModalidadAgregada').isin(modalidades))\n",
    "\n",
    "# Definimos una lista de decisiones que queremos filtrar en el DataFrame.\n",
    "#decidiones = ['Aprobado', 'Desistido','Aclarado','Prorrogado','Negado','Recurso Confirmado', 'Recurso Rechazado', 'Recurso Aclarado', 'Renuncia de Licencia', 'Revocado', 'Archivado']\n",
    "#decidiones = ['Aprobado', 'Desistido','Renuncia de Licencia']\n",
    "\n",
    "# Filtramos el DataFrame df_licencias para mantener solo las filas donde 'Tipo de decisión' esté en la lista decidiones.\n",
    "#df_licencias = df_licencias.filter(col('Tipo de decisión').isin(decidiones))\n",
    "\n",
    "# Filtramos el DataFrame df_licencias para mantener solo las filas donde 'Conteo licencias' sea mayor a 0.\n",
    "df_licencias = df_licencias.filter(col('Conteo licencias') > 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da379dcf-dc9a-4180-b38d-9500855e10b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupamos el DataFrame 'predios_agregados_df' por las columnas 'LOTLOTE_ID' y 'año_vigencia'.\n",
    "# Luego, aplicamos varias funciones de agregación:\n",
    "# - Sumamos los valores de la columna 'AREA_TERRENO'.\n",
    "# - Sumamos los valores de la columna 'AREA_CONSTRUIDA'.\n",
    "# - Sumamos los valores de la columna 'VALOR_AVALUO'.\n",
    "# - Contamos el número de valores en la columna 'CHIP'.\n",
    "# El resultado es un DataFrame que tiene una columna para cada función de agregación aplicada.\n",
    "conteo_licencias = df_licencias.groupBy([\"Código lote\",\"Año de ejecutoría\"]).agg({\"Área\":\"sum\", \"Unidades\":\"sum\", \"Matrícula Inmobiliaria\":\"count\", \"CHIP\":\"count\"}).fillna(0)\n",
    "## Matrícula Inmobiliaria, CHIP, Unidades\tÁrea\n",
    "# Renombramos las columnas del DataFrame 'conteo_predios' para que tengan nombres más descriptivos y fáciles de entender.\n",
    "# - \"count(CHIP)\" se renombra a \"NumPredios\".\n",
    "# - \"sum(VALOR_AVALUO)\" se renombra a \"ValorAvaluo\".\n",
    "# - \"sum(AREA_CONSTRUIDA)\" se renombra a \"AC\".\n",
    "# - \"sum(AREA_TERRENO)\" se renombra a \"AT\".\n",
    "conteo_licencias = conteo_licencias.withColumnRenamed(\"count(CHIP)\",\"NumCHIP\") \\\n",
    "    .withColumnRenamed(\"sum(Área)\",\"Area\") \\\n",
    "    .withColumnRenamed(\"sum(Unidades)\",\"Unidades\") \\\n",
    "    .withColumnRenamed(\"count(Matrícula Inmobiliaria)\",\"Matriculas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "095e5132-7772-438c-8938-bc51a0aafd31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_licencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acc583ed-7b4b-4874-a04d-c3d58c85a35b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupamos el DataFrame df_licencias por las columnas \"Código lote\" y \"Año de ejecutoría\".\n",
    "# Luego, pivotamos el DataFrame usando la columna \"ModalidadAgregada\" para transformar sus valores únicos en columnas individuales.\n",
    "# Posteriormente, contamos las ocurrencias de cada combinación de \"Código lote\", \"Año de ejecutoría\" y \"ModalidadAgregada\".\n",
    "# Finalmente, reemplazamos cualquier valor nulo con 0 usando fillna(0).\n",
    "conteo_modalidades = (df_licencias.groupBy([\"Código lote\",\"Año de ejecutoría\"]).pivot(\"ModalidadAgregada\").count()).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aad7904d-953d-44e7-a6a4-9039b4bbedb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Agrupamos el DataFrame df_licencias por las columnas \"Código lote\" y \"Año de ejecutoría\".\n",
    "# Luego, pivotamos el DataFrame usando la columna \"ModalidadAgregada\" para transformar sus valores únicos en columnas individuales.\n",
    "# Posteriormente, contamos las ocurrencias de cada combinación de \"Código lote\", \"Año de ejecutoría\" y \"ModalidadAgregada\".\n",
    "# Finalmente, reemplazamos cualquier valor nulo con 0 usando fillna(0).\n",
    "conteo_usos_licencias = (df_licencias.groupBy([\"Código lote\",\"Año de ejecutoría\"]).pivot(\"UsoAgregado\").count()).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1c717ff-1fa3-4945-9079-51fc55713d81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(conteo_usos_licencias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a561c29-9e25-4487-866d-c94ade902ad4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertimos el DataFrame de Spark (df_licencias) a un DataFrame de Pandas (df_licencias_pd).\n",
    "df_licencias_pd = df_licencias.toPandas()\n",
    "\n",
    "# Filtramos el DataFrame de Pandas para mantener solo las filas donde 'def_lat' no es nulo.\n",
    "df_licencias_pd = df_licencias_pd[df_licencias_pd['def_lat'].notnull()]\n",
    "\n",
    "# Filtramos el DataFrame de Pandas para mantener solo las filas donde 'def_lng' no es nulo.\n",
    "df_licencias_pd = df_licencias_pd[df_licencias_pd['def_lng'].notnull()]\n",
    "\n",
    "# Creamos una nueva columna 'geometry' en el DataFrame de Pandas.\n",
    "# Esta columna contiene objetos Point (puntos geográficos) creados a partir de las columnas 'def_lng' y 'def_lat'.\n",
    "df_licencias_pd['geometry'] = df_licencias_pd.apply(lambda row: Point(row['def_lng'], row['def_lat']), axis=1)\n",
    "\n",
    "# Convertimos el DataFrame de Pandas (df_licencias_pd) a un GeoDataFrame (licencias_gdf) usando geopandas.\n",
    "# Especificamos la columna 'geometry' como la columna que contiene las geometrías y definimos el sistema de referencia de coordenadas (CRS) como \"EPSG:4326\".\n",
    "licencias_gdf = gpd.GeoDataFrame(df_licencias_pd, geometry='geometry', crs=\"EPSG:4326\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99eea677-b2b1-4387-8ea1-903e32fd18f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertimos el DataFrame de Spark (isocrona_df) a un DataFrame de Pandas (isocronas_pd).\n",
    "isocrona_df = isocrona_df.withColumn(\"geometry\", col(\"geometry\").cast(\"string\"))\n",
    "isocronas_pd = isocrona_df.toPandas()\n",
    "isocronas_pd['geometry'] = isocronas_pd['geometry'].apply(wkt.loads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8fee477-4fd3-44ce-b327-43efff8ccf81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convertimos el DataFrame de Pandas (isocronas_pd) a un GeoDataFrame (isocronas_gdf) usando geopandas.\n",
    "# Especificamos la columna 'geometry' como la columna que contiene las geometrías y definimos el sistema de referencia de coordenadas (CRS) como \"EPSG:4326\".\n",
    "isocronas_gdf = gpd.GeoDataFrame(isocronas_pd, geometry='geometry', crs=\"EPSG:4326\")\n",
    "\n",
    "# Realizamos una operación de join espacial entre los GeoDataFrames licencias_gdf e isocronas_gdf.\n",
    "# El join se realiza con base en la relación espacial \"within\", es decir, se unen las filas de licencias_gdf que están dentro de las geometrías de isocronas_gdf.\n",
    "# El resultado es un nuevo GeoDataFrame (licencias_gdf_iso) que contiene solo las licencias que están dentro de las isócronas.\n",
    "licencias_gdf_iso = gpd.sjoin(licencias_gdf, isocronas_gdf, how=\"inner\", op=\"within\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "248f060f-7463-41ef-93cd-b3d8a0891085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "licencias_gdf_iso[['geometry','Tiempo']].sample(5000).explore('Tiempo', tiles='CartoDB positron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca3b56d-40df-4d2f-b158-42422f3ec70b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "campos_licencias = ['Código lote','Año de ejecutoría','Localidad','UPZ','Tiempo','layer','Linea','Tipo de decisión','def_lng','def_lat']\n",
    "licencias_gdf = pd.DataFrame(licencias_gdf_iso)[campos_licencias].drop_duplicates()\n",
    "conteo_licencias_df = conteo_licencias.toPandas().drop_duplicates()\n",
    "conteo_modalidades_df = conteo_modalidades.toPandas().drop_duplicates()\n",
    "conteo_usos_licencias_df = conteo_usos_licencias.toPandas().drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73b732f2-b568-4676-be65-6948de5a1e00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Realizamos una operación de unión (merge) entre el GeoDataFrame licencias_gdf_iso y el DataFrame conteo_modalidades.\n",
    "# La unión se basa en las columnas 'Código lote' y 'Año de ejecutoría'.\n",
    "# Utilizamos un join de tipo \"left\", lo que significa que se conservarán todas las filas del GeoDataFrame licencias_gdf_iso y se agregarán las columnas correspondientes del DataFrame conteo_modalidades.\n",
    "# Si no hay coincidencia para una fila en particular de licencias_gdf_iso en conteo_modalidades, los valores de las columnas agregadas serán NaN.\n",
    "union_licencias_df =  pd.merge(left=licencias_gdf, right=conteo_licencias_df, on=['Código lote','Año de ejecutoría'], how=\"left\")\n",
    "#union_licencias_df =  pd.merge(left=union_licencias_df, right=conteo_modalidades_df, on=['Código lote','Año de ejecutoría'], how=\"left\")\n",
    "union_licencias_df =  pd.merge(left=union_licencias_df, right=conteo_usos_licencias_df, on=['Código lote','Año de ejecutoría'], how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5612bc7f-6f88-4d22-ada7-948aa065ed08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_licencias_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "158f0385-5b26-4252-8c8b-6582397eb5d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#union_licencias_df.columns = ['CodigoLote','AnioDeEjecutoria','Localidad','UPZ','Tiempo','Estacion','Linea','TipoDeDecision','DefLng','DefLat','NumCHIP','Area','Unidades','Matriculas','Adecuacion','Ampliacion','Cerramiento','CulminacionDeObras','Demolicion','Modificacion','ObraNueva','Otras','PropiedadHorizontal','Reconocimiento','ReforzamientoEstructural','Restauración','SinModalidad','Subdivisión']\n",
    "union_licencias_df.columns = ['CodigoLote','AnioDeEjecutoria','Localidad','UPZ','Tiempo','Estacion','Linea','TipoDeDecision','DefLng','DefLat','NumCHIP','Area','Unidades','Matriculas','ComercioYServicios','Industrial','Otros','Residencial']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9810e50f-15b6-4d42-82d0-c10d6f835505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convertir el DataFrame de Pandas a un DataFrame de PySpark\n",
    "union_licencias_spark_df = spark.createDataFrame(union_licencias_df)\n",
    "\n",
    "# Escribir el DataFrame de PySpark a un archivo CSV\n",
    "union_licencias_spark_df.write.csv('/FileStore/tables/SDP/union_licencias_2008_2023.csv', header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6b5d3120-4f5e-4dd5-a5e5-ebbaa8e72354",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_licencias_spark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982ca001-6943-485f-a5c5-d36162eeef35",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "union_licencias_spark_df.createOrReplaceTempView(\"union_sdp_spark_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a04d607-7d78-4475-b089-54d90c093e06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE lotes_licencias_sdp AS (\n",
    "  Select \n",
    "  *, cast(AnioDeEjecutoria as float) as AnioVigencia\n",
    "  from union_sdp_spark_df where CodigoLote is not null);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "847ed319-d285-46e5-a93a-8d2185c9de6c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE lotes_catastro AS (\n",
    "  Select \n",
    "  *\n",
    "  from lotes_catastro where Estrato <> '0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "728a39f6-afec-49d0-abc4-0a593cf0ae74",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT AnioDeEjecutoria, sum(ComercioYServicios),sum(Industrial), sum(Otros), sum(Residencial) from (select *\n",
    " from  lotes_licencias_sdp) group by AnioDeEjecutoria ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35136504-21c1-4844-b660-8318cb1eb798",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT * FROM lotes_catastro;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d58210af-74e7-48ef-905a-c478d563696e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201ca018-f4f2-4c76-b483-15c445c53814",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#dbfs:/FileStore/tables/Catastro/2010/predios_2010.csv\n",
    "df = spark.read.format('csv').options(header='true').load('/FileStore/tables/Catastro/2023/predios_2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f57dd8b8-5d1b-4a71-9c57-b6e68ed4fe8e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fddf70dc-8308-4c6e-87e8-b68980a094b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4485955173114332,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Preparación datos - Tablero Construcción",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
